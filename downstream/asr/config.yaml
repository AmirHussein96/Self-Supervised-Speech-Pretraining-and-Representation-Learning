runner:
  total_steps: 1000000
  gradient_clipping: 1
  gradient_accumulate_steps: 2

  log_step: 100
  eval_step: 1000
  save_step: 100
  max_keep: 1
  eval_dataloaders:
    - dev-clean
    - dev-other

optimizer:
  name: TorchOptim
  torch_optim_name: Adam
  lr: 1.0e-4

# comment the whole scheduler config block
# to disable learning rate scheduling
# scheduler:
#   name: linear_schedule_with_warmup
#   num_warmup_steps: 1400

# comment the whole specaug config block
# to disable specaug on representation
# specaug:
#   apply_time_warp: true
#   apply_time_mask: true
#   apply_freq_mask: true

downstream_expert:
  datarc:
    train: ['train-clean-100']
    dev-clean: ['dev-clean']
    dev-other: ['dev-other']
    test-clean: ['test-clean']
    test-other: ['test-other']

    num_workers: 8
    train_batch_size: 16
    eval_batch_size: 16
    libri_root: '/path/to/LibriSpeech'
    bucket_file: 'data/librispeech/len_for_bucket'

    zero_infinity: True

  modelrc:
    project_dim: 1024
    select: RNNs
    Wav2Letter:
      total_rate: 320
    RNNs:
      total_rate: 320
      module: 'LSTM'                        # 'LSTM'/'GRU'
      bidirection: True
      dim: [1024, 1024, 1024]
      dropout: [0.2, 0.2, 0.2]
      layer_norm: [True, True, True]
      proj: [True, True, True]              # Linear projection + Tanh after each rnn layer
      sample_rate: [1, 1, 1]
      sample_style: 'concat'                  # 'drop'/'concat'
